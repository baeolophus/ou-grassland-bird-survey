#microbenchmark (runtime) vs accuracy comparisons between statewide and spatially explicit models

#required libraries
library(dplyr)
library(lme4)
library(lmerTest)

#set working directory as appropriate.

setwd("E:/Documents/college/OU-postdoc/research/grassland_bird_surveys/ougrassland/ensemble_results/Current")
#original models

setwd("E:/Documents/college/OU-postdoc/research/grassland_bird_surveys/ougrassland/ensemble_results/Downscale_current")
#downscaled models


#list of species
specieslist <- c("NOBO",
                 "UPSA",
                 "HOLA",
                 "CASP",
                 "FISP",
                 "LASP",
                 "GRSP",
                 "DICK",
                 "EAME",
                 "WEME",
                 "BHCO")

#function to load microbenchmark results as generated by "run_from_here_ensemble.R"
microbenchmarkcsv <- function (SPECIES) {
microbenchmarks <- read.csv(paste0(SPECIES,
                                   "/",
                                   SPECIES,
                                   "_products_microbenchmarks.csv"))
return(microbenchmarks)
}

#Load all species from list of species.
listofmb <- lapply (specieslist,
                    FUN = microbenchmarkcsv)

#Turn the loaded list into a data frame.
mb.df <- do.call(rbind, lapply(listofmb,
                               data.frame, 
                               stringsAsFactors=FALSE))

#Separate out the scale (statewide, spatially explicit ones) and runtype (tree == 2 vs raster==1 prediction)
mb.df.sep <- separate(mb.df,
                          into = c("scale",
                                   "runtype"),
                          model,
                      sep = -2)

#turn characters into factor and order them appropriately.
mb.df.sep$scale <- factor(mb.df.sep$scale,
                          levels = c("statewide",
                                     "large",
                                     "medium",
                                     "small"))

#group and summarize runtime.
mb.summed <- group_by(mb.df.sep,
                      scale,
                      Species) %>%
  summarize("runtime" = sum(time/1000000000)) #convert to seconds

#convert to hours and days.
mb.summed$runtimehrs <- mb.summed$runtime/(60*60)
mb.summed$runtimedays <- mb.summed$runtimehrs/24


#Get statewide runtimes for ratios of spatially explicit vs statewide models later.
statewide.values <-filter (mb.summed,
                            scale == "statewide") %>%
  ungroup(.) %>%
  dplyr::select(., runtime)

mb.summed$statewide <- rep(statewide.values$runtime, 4)

#create ratio of spatially explicit model runtime to statewide model runtime.
mb.summed$ratio <- mb.summed$runtime/mb.summed$statewide

#are runtimes different?
runtime.model <- lmer(runtimehrs ~ scale+(1|Species),
                      data = mb.summed)
plot(runtime.model)
summary(runtime.model)
anova(runtime.model)

boxplot(runtimehrs ~ scale,
        data = mb.summed,
        ylab = "Runtime in hours",
        xlab = "Scale",
        lwd = 2,
        cex.axis = 2)

hist(resid(runtime.model))

summary(mb.summed[mb.summed$scale != "statewide", "ratio"])

#add in evaluation results

eval.function <- function(SPECIES) {
evalresults <- readRDS(file.path(
  SPECIES,
  paste0(
    SPECIES,
    "_products_evaluation_results"))
)
#contains AUC/RMSE for all.
eval.df <- data.frame(evalresults)

listerrornames <- c("statewide.sampling.rmse.sameyear",
                    "statewide.sampling.auc.sameyear",
                    "small.sampling.rmse.sameyear",
                    "small.sampling.auc.sameyear",
                    "medium.sampling.rmse.sameyear",
                    "medium.sampling.auc.sameyear",
                    "large.sampling.rmse.sameyear",
                    "large.sampling.auc.sameyear",
                    "statewide.sampling.rmse.diffyear",
                    "statewide.sampling.auc.diffyear",
                    "small.sampling.rmse.diffyear",
                    "small.sampling.auc.diffyear",
                    "medium.sampling.rmse.diffyear",
                    "medium.sampling.auc.diffyear",
                    "large.sampling.rmse.diffyear",
                    "large.sampling.auc.diffyear")
colnames(eval.df) <- listerrornames

eval.df.sep <- gather_(eval.df,
                       key_col = "errordescriptor",
                       value_col = "errornum",
                       gather_cols = listerrornames) %>%
  separate_(col = "errordescriptor",
            into = c("scale",
                     "sampling",
                     "errortype",
                     "year"))

eval.df.sep$scale <- factor(eval.df.sep$scale,
                          levels = c("statewide",
                                     "large",
                                     "medium",
                                     "small"))

eval.df.sep$species <- SPECIES
return(eval.df.sep)
}

listofeval <- lapply (specieslist,
                    FUN = eval.function)
evalsforbinding <- do.call(rbind,
                 lapply(listofeval, 
                               data.frame, 
                               stringsAsFactors=FALSE))

final.models.and.plots <- function (errortypehere,
                                    yearhere) {
  
#filter to get the year and error (auc or rmse)
evalsforbinding.filtered1 <- dplyr::filter(evalsforbinding,
                                         errortype == errortypehere,
                                         year == yearhere)


evalsforbinding.filtered <- group_by(evalsforbinding.filtered1,
           scale,
           species) %>%
  summarize(errornum = mean(errornum))

joined <- left_join(x = evalsforbinding.filtered,
                         y = mb.summed,
                         by = c("scale" = "scale",
                                "species" = "Species"))


#run models

error.runtime <- lmer(errornum ~ runtimehrs + (1|species),
                    data = joined)

plot(error.runtime)
summary(error.runtime)
hist(resid(error.runtime))
runtimeplot <- plot(errornum ~runtimehrs,
     data = joined, 
     xlab = "Runtime (hrs)",
     ylab = paste(errortypehere,
                  yearhere,
                  sep = " "))


error.scale <- lmer(errornum ~ scale + (1|species),
                    data = joined)
plot(error.scale)
summary(error.scale)
hist(resid(error.scale))

#levels for appropriate graphs

return(list(error.runtime,
            error.scale,
            joined))

}

(auc.same <- final.models.and.plots("auc",
                       "sameyear"))

(auc.diff <- final.models.and.plots("auc",
                                "diffyear"))


(rmse.same <- final.models.and.plots("rmse",
                                "sameyear"))


(rmse.diff <- final.models.and.plots("rmse",
                                "diffyear"))


library(car)
summary(auc.same[[1]])
summary(auc.diff[[1]])
summary(rmse.same[[1]])
summary(rmse.diff[[1]])

par(mfrow=c(2,2),
    mar=c(5, 6, 4, 2))
boxplot(errornum ~ scale,
                     data = auc.same[[3]],
                     ylab = "AUC",
                     main = "Same year",
        ylim = c(0.4, 1))
abline(h=0.5,
       lty = "dashed")
boxplot(errornum ~ scale,
        data = auc.diff[[3]],
        main = "Different year",
        ylim = c(0.4, 1))
abline(h=0.5,
       lty = "dashed")
boxplot(errornum ~ scale,
        data = rmse.same[[3]],
        ylab = "RMSE",
        ylim = c(0.1, 0.5))
boxplot(errornum ~ scale,
        data = rmse.diff[[3]],
        ylim = c(0.1, 0.5))

#Function to get prediction lines for figure.
prediction.function<-function(behaviormerModobject,
                              dataname){
  nd<-data.frame("runtimehrs"=seq(min(dataname[[3]]$runtimehrs,
                                    na.rm=TRUE),
                                max(dataname[[3]]$runtimehrs,
                                    na.rm=TRUE),
                                length.out=length(dataname[[3]]$runtimehrs)))
  predicted<-data.frame("y"=predict(behaviormerModobject,
                                    newdata=nd,
                                    type="response",
                                    re.form=NA),#do not condition on random effects
                        nd) 
}

#Fig. 5, runtime vs error
par(mfrow=c(2,2),
    mar=c(5, 6, 4, 2))
plot(errornum ~ runtimehrs,
        data = auc.same[[3]],
        ylab = "AUC",
        main = "Same year",
     xlab="",
        ylim = c(0.4, 1))
abline(h=0.5,
       lty = "dashed")
plot(errornum ~ runtimehrs,
        data = auc.diff[[3]],
        main = "Different year",
     
     ylab = "",
     xlab="",
        ylim = c(0.4, 1))
abline(h=0.5,
       lty = "dashed")
plot(errornum ~ runtimehrs,
        data = rmse.same[[3]],
        ylab = "RMSE",
     xlab="Runtime hours")
plot(errornum ~ runtimehrs,
        data = rmse.diff[[3]],
     xlab="Runtime hours",
     ylab = "",
     main = expression(paste(beta==0.00026, ",", ~p==0.0068)))
rmse.diff.line <- prediction.function(rmse.diff[[1]],
                    rmse.diff)
graphics::lines(x = rmse.diff.line$runtimehrs,
                y = rmse.diff.line$y,
                lty = "solid")

#showing that different years are less predicted than same years.

auc.same[[3]]$year <- "same"
auc.diff[[3]]$year <- "diff"

auc.years <- data.frame(rbind(auc.same[[3]],
                              auc.diff[[3]]))

years.auc <- lmer(errornum ~ year + (1|species), 
                  data = auc.years)

summary(years.auc, type = 2)

plot(rmse.years$errornum ~ as.factor(rmse.years$year))

rmse.same[[3]]$year <- "same"
rmse.diff[[3]]$year <- "diff"

rmse.years <- data.frame(rbind(rmse.same[[3]],
                              rmse.diff[[3]]))

years.rmse <- lmer(errornum ~ year + (1|species), 
                  data = rmse.years)

summary(years.rmse, type = 2)

